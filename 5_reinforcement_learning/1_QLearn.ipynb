{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2502cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb2398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSize = 4\n",
    "\n",
    "#(0,0) (0,1) (0,2) (0,3)\n",
    "#(1,0) (1,1) (1,2) (1,3)\n",
    "#(2,0) (2,1) (2,2) (2,3)\n",
    "#(3,0) (3,1) (3,2) (3,3)\n",
    "Goal = (3, 3)\n",
    "Trap = ((0, 2), (2, 1))\n",
    "ActionsList = ['UP','DOWN','LEFT','RIGHT']\n",
    "\n",
    "\n",
    "Q = np.zeros((GridSize, GridSize, 4)) # Array for Q-Table\n",
    "\n",
    "\n",
    "Alpha = 0.5 \n",
    "Gamma = 0.9\n",
    "Epsilon = 0.2\n",
    "Episodes = 40\n",
    "Delay = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee4f155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1\n",
      "State (0, 0), Action LEFT, Reward -1, Next (0, 0), Exploring, Q 0.00 → -0.50\n",
      "State (0, 0), Action UP, Reward -1, Next (0, 0), Exploiting, Q 0.00 → -0.50\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 0.00 → -0.50\n",
      "State (1, 0), Action LEFT, Reward -1, Next (1, 0), Exploring, Q 0.00 → -0.50\n",
      "State (1, 0), Action UP, Reward -1, Next (0, 0), Exploiting, Q 0.00 → -0.50\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploiting, Q 0.00 → -0.50\n",
      "State (0, 1), Action RIGHT, Reward -10, Next (0, 2), Exploring, Q 0.00 → -5.00\n",
      "\n",
      "Episode 2\n",
      "State (0, 0), Action UP, Reward -1, Next (0, 0), Exploiting, Q -0.50 → -0.97\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -0.50 → -0.75\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 0.00 → -0.50\n",
      "State (2, 0), Action LEFT, Reward -1, Next (2, 0), Exploring, Q 0.00 → -0.50\n",
      "State (2, 0), Action UP, Reward -1, Next (1, 0), Exploiting, Q 0.00 → -0.50\n",
      "State (1, 0), Action RIGHT, Reward -1, Next (1, 1), Exploiting, Q 0.00 → -0.50\n",
      "State (1, 1), Action UP, Reward -1, Next (0, 1), Exploring, Q 0.00 → -0.50\n",
      "State (0, 1), Action UP, Reward -1, Next (0, 1), Exploiting, Q 0.00 → -0.50\n",
      "State (0, 1), Action DOWN, Reward -1, Next (1, 1), Exploiting, Q 0.00 → -0.50\n",
      "State (1, 1), Action DOWN, Reward -10, Next (2, 1), Exploiting, Q 0.00 → -5.00\n",
      "\n",
      "Episode 3\n",
      "State (0, 0), Action LEFT, Reward -1, Next (0, 0), Exploiting, Q -0.50 → -0.97\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploiting, Q -0.50 → -0.75\n",
      "State (0, 1), Action LEFT, Reward -1, Next (0, 0), Exploiting, Q 0.00 → -0.84\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploring, Q -0.75 → -1.10\n",
      "State (0, 1), Action UP, Reward -1, Next (0, 1), Exploiting, Q -0.50 → -0.97\n",
      "State (0, 1), Action UP, Reward -1, Next (0, 1), Exploring, Q -0.97 → -1.21\n",
      "State (0, 1), Action DOWN, Reward -1, Next (1, 1), Exploiting, Q -0.50 → -0.75\n",
      "State (1, 1), Action LEFT, Reward -1, Next (1, 0), Exploiting, Q 0.00 → -0.72\n",
      "State (1, 0), Action UP, Reward -1, Next (0, 0), Exploiting, Q -0.50 → -1.09\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -0.75 → -1.10\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q -0.50 → -0.75\n",
      "State (2, 0), Action UP, Reward -1, Next (1, 0), Exploring, Q -0.50 → -0.97\n",
      "State (1, 0), Action LEFT, Reward -1, Next (1, 0), Exploiting, Q -0.50 → -0.97\n",
      "State (1, 0), Action RIGHT, Reward -1, Next (1, 1), Exploiting, Q -0.50 → -0.75\n",
      "State (1, 1), Action RIGHT, Reward -1, Next (1, 2), Exploiting, Q 0.00 → -0.50\n",
      "State (1, 2), Action LEFT, Reward -1, Next (1, 1), Exploring, Q 0.00 → -0.72\n",
      "State (1, 1), Action UP, Reward -1, Next (0, 1), Exploiting, Q -0.50 → -1.09\n",
      "State (0, 1), Action DOWN, Reward -1, Next (1, 1), Exploiting, Q -0.75 → -1.10\n",
      "State (1, 1), Action RIGHT, Reward -1, Next (1, 2), Exploiting, Q -0.50 → -0.75\n",
      "State (1, 2), Action UP, Reward -10, Next (0, 2), Exploiting, Q 0.00 → -5.00\n",
      "\n",
      "Episode 4\n",
      "State (0, 0), Action UP, Reward -1, Next (0, 0), Exploiting, Q -0.97 → -1.43\n",
      "State (0, 0), Action LEFT, Reward -1, Next (0, 0), Exploiting, Q -0.97 → -1.43\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -1.10 → -1.39\n",
      "State (1, 0), Action LEFT, Reward -1, Next (1, 0), Exploring, Q -0.97 → -1.32\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q -0.75 → -0.88\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 0.00 → -0.50\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploring, Q 0.00 → -0.50\n",
      "State (3, 1), Action DOWN, Reward -1, Next (3, 1), Exploring, Q 0.00 → -0.50\n",
      "State (3, 1), Action UP, Reward -10, Next (2, 1), Exploiting, Q 0.00 → -5.00\n",
      "\n",
      "Episode 5\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploiting, Q -1.10 → -1.43\n",
      "State (0, 1), Action LEFT, Reward -1, Next (0, 0), Exploiting, Q -0.84 → -1.54\n",
      "State (0, 0), Action LEFT, Reward -1, Next (0, 0), Exploring, Q -1.43 → -1.84\n",
      "State (0, 0), Action UP, Reward -1, Next (0, 0), Exploring, Q -1.43 → -1.84\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -1.39 → -1.53\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploring, Q -0.88 → -0.94\n",
      "State (2, 0), Action RIGHT, Reward -10, Next (2, 1), Exploiting, Q 0.00 → -5.00\n",
      "\n",
      "Episode 6\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploiting, Q -1.43 → -1.71\n",
      "State (0, 1), Action DOWN, Reward -1, Next (1, 1), Exploiting, Q -1.10 → -1.38\n",
      "State (1, 1), Action LEFT, Reward -1, Next (1, 0), Exploiting, Q -0.72 → -1.20\n",
      "State (1, 0), Action RIGHT, Reward -1, Next (1, 1), Exploiting, Q -0.75 → -1.21\n",
      "State (1, 1), Action RIGHT, Reward -1, Next (1, 2), Exploiting, Q -0.75 → -0.88\n",
      "State (1, 2), Action DOWN, Reward -1, Next (2, 2), Exploiting, Q 0.00 → -0.50\n",
      "State (2, 2), Action UP, Reward -1, Next (1, 2), Exploring, Q 0.00 → -0.50\n",
      "State (1, 2), Action RIGHT, Reward -1, Next (1, 3), Exploiting, Q 0.00 → -0.50\n",
      "State (1, 3), Action LEFT, Reward -1, Next (1, 2), Exploring, Q 0.00 → -0.72\n",
      "State (1, 2), Action DOWN, Reward -1, Next (2, 2), Exploiting, Q -0.50 → -0.75\n",
      "State (2, 2), Action DOWN, Reward -1, Next (3, 2), Exploiting, Q 0.00 → -0.50\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploring, Q 0.00 → 50.00\n",
      "\n",
      "Episode 7\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -1.53 → -1.69\n",
      "State (1, 0), Action RIGHT, Reward -1, Next (1, 1), Exploring, Q -1.21 → -1.50\n",
      "State (1, 1), Action RIGHT, Reward -1, Next (1, 2), Exploiting, Q -0.88 → -1.16\n",
      "State (1, 2), Action RIGHT, Reward -1, Next (1, 3), Exploiting, Q -0.50 → -0.75\n",
      "State (1, 3), Action UP, Reward -1, Next (0, 3), Exploiting, Q 0.00 → -0.50\n",
      "State (0, 3), Action RIGHT, Reward -1, Next (0, 3), Exploring, Q 0.00 → -0.50\n",
      "State (0, 3), Action UP, Reward -1, Next (0, 3), Exploiting, Q 0.00 → -0.50\n",
      "State (0, 3), Action DOWN, Reward -1, Next (1, 3), Exploiting, Q 0.00 → -0.50\n",
      "State (1, 3), Action DOWN, Reward -1, Next (2, 3), Exploiting, Q 0.00 → -0.50\n",
      "State (2, 3), Action RIGHT, Reward -1, Next (2, 3), Exploring, Q 0.00 → -0.50\n",
      "State (2, 3), Action UP, Reward -1, Next (1, 3), Exploiting, Q 0.00 → -0.50\n",
      "State (1, 3), Action RIGHT, Reward -1, Next (1, 3), Exploiting, Q 0.00 → -0.50\n",
      "State (1, 3), Action UP, Reward -1, Next (0, 3), Exploiting, Q -0.50 → -0.75\n",
      "State (0, 3), Action LEFT, Reward -10, Next (0, 2), Exploiting, Q 0.00 → -5.00\n",
      "\n",
      "Episode 8\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -1.69 → -1.77\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q -0.94 → -1.19\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q -0.50 → -0.75\n",
      "State (3, 0), Action UP, Reward -1, Next (2, 0), Exploiting, Q 0.00 → -0.72\n",
      "State (2, 0), Action LEFT, Reward -1, Next (2, 0), Exploiting, Q -0.50 → -0.97\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q -0.75 → -0.88\n",
      "State (3, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 0.00 → -0.50\n",
      "State (3, 0), Action LEFT, Reward -1, Next (3, 0), Exploiting, Q 0.00 → -0.50\n",
      "State (3, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q -0.50 → -0.97\n",
      "State (3, 0), Action LEFT, Reward -1, Next (3, 0), Exploiting, Q -0.50 → -0.97\n",
      "State (3, 0), Action LEFT, Reward -1, Next (3, 0), Exploring, Q -0.97 → -1.21\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q -0.50 → -0.75\n",
      "State (3, 1), Action LEFT, Reward -1, Next (3, 0), Exploiting, Q 0.00 → -0.83\n",
      "State (3, 0), Action LEFT, Reward -1, Next (3, 0), Exploring, Q -1.21 → -1.43\n",
      "State (3, 0), Action UP, Reward -1, Next (2, 0), Exploiting, Q -0.72 → -1.26\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q -0.88 → -1.27\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q -0.75 → -0.88\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 0.00 → 22.00\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 50.00 → 75.00\n",
      "\n",
      "Episode 9\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploiting, Q -1.71 → -1.90\n",
      "State (0, 1), Action LEFT, Reward -1, Next (0, 0), Exploring, Q -1.54 → -2.07\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -1.77 → -1.87\n",
      "State (1, 0), Action UP, Reward -1, Next (0, 0), Exploiting, Q -1.09 → -1.87\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploring, Q -1.87 → -1.97\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q -1.19 → -1.54\n",
      "State (2, 0), Action RIGHT, Reward -10, Next (2, 1), Exploring, Q -5.00 → -7.50\n",
      "\n",
      "Episode 10\n",
      "State (0, 0), Action UP, Reward -1, Next (0, 0), Exploiting, Q -1.84 → -2.25\n",
      "State (0, 0), Action LEFT, Reward -1, Next (0, 0), Exploiting, Q -1.84 → -2.25\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploring, Q -1.97 → -2.08\n",
      "State (1, 0), Action LEFT, Reward -1, Next (1, 0), Exploiting, Q -1.32 → -1.76\n",
      "State (1, 0), Action RIGHT, Reward -1, Next (1, 1), Exploiting, Q -1.50 → -1.74\n",
      "State (1, 1), Action UP, Reward -1, Next (0, 1), Exploiting, Q -1.09 → -1.59\n",
      "State (0, 1), Action UP, Reward -1, Next (0, 1), Exploiting, Q -1.21 → -1.65\n",
      "State (0, 1), Action DOWN, Reward -1, Next (1, 1), Exploiting, Q -1.38 → -1.71\n",
      "State (1, 1), Action RIGHT, Reward -1, Next (1, 2), Exploiting, Q -1.16 → -1.41\n",
      "State (1, 2), Action LEFT, Reward -1, Next (1, 1), Exploiting, Q -0.72 → -1.40\n",
      "State (1, 1), Action LEFT, Reward -1, Next (1, 0), Exploiting, Q -1.20 → -1.79\n",
      "State (1, 0), Action LEFT, Reward -1, Next (1, 0), Exploring, Q -1.76 → -2.07\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploring, Q -1.54 → -1.71\n",
      "State (2, 0), Action UP, Reward -1, Next (1, 0), Exploiting, Q -0.97 → -1.76\n",
      "State (1, 0), Action LEFT, Reward -1, Next (1, 0), Exploring, Q -2.07 → -2.30\n",
      "State (1, 0), Action LEFT, Reward -1, Next (1, 0), Exploring, Q -2.30 → -2.42\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q -1.71 → -1.79\n",
      "State (2, 0), Action LEFT, Reward -1, Next (2, 0), Exploiting, Q -0.97 → -1.43\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q -1.27 → -1.53\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q -0.88 → 8.96\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 22.00 → 44.25\n",
      "State (3, 2), Action UP, Reward -1, Next (2, 2), Exploring, Q 0.00 → -0.50\n",
      "State (2, 2), Action RIGHT, Reward -1, Next (2, 3), Exploring, Q 0.00 → -0.50\n",
      "State (2, 3), Action DOWN, Reward 100, Next (3, 3), Exploiting, Q 0.00 → 50.00\n",
      "\n",
      "Episode 11\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploiting, Q -1.90 → -2.19\n",
      "State (0, 1), Action UP, Reward -1, Next (0, 1), Exploiting, Q -1.65 → -2.07\n",
      "State (0, 1), Action DOWN, Reward -1, Next (1, 1), Exploiting, Q -1.71 → -1.99\n",
      "State (1, 1), Action RIGHT, Reward -1, Next (1, 2), Exploiting, Q -1.41 → -1.54\n",
      "State (1, 2), Action DOWN, Reward -1, Next (2, 2), Exploiting, Q -0.75 → -0.88\n",
      "State (2, 2), Action LEFT, Reward -10, Next (2, 1), Exploiting, Q 0.00 → -5.00\n",
      "\n",
      "Episode 12\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -2.08 → -2.32\n",
      "State (1, 0), Action RIGHT, Reward -1, Next (1, 1), Exploiting, Q -1.74 → -2.06\n",
      "State (1, 1), Action RIGHT, Reward -1, Next (1, 2), Exploiting, Q -1.54 → -1.61\n",
      "State (1, 2), Action RIGHT, Reward -1, Next (1, 3), Exploiting, Q -0.75 → -1.10\n",
      "State (1, 3), Action DOWN, Reward -1, Next (2, 3), Exploiting, Q -0.50 → 21.75\n",
      "State (2, 3), Action UP, Reward -1, Next (1, 3), Exploring, Q -0.50 → 9.04\n",
      "State (1, 3), Action DOWN, Reward -1, Next (2, 3), Exploiting, Q 21.75 → 32.88\n",
      "State (2, 3), Action DOWN, Reward 100, Next (3, 3), Exploiting, Q 50.00 → 75.00\n",
      "\n",
      "Episode 13\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploiting, Q -2.19 → -2.49\n",
      "State (0, 1), Action DOWN, Reward -1, Next (1, 1), Exploiting, Q -1.99 → -2.21\n",
      "State (1, 1), Action UP, Reward -1, Next (0, 1), Exploring, Q -1.59 → -2.22\n",
      "State (0, 1), Action LEFT, Reward -1, Next (0, 0), Exploring, Q -2.07 → -2.54\n",
      "State (0, 0), Action UP, Reward -1, Next (0, 0), Exploiting, Q -2.25 → -2.63\n",
      "State (0, 0), Action LEFT, Reward -1, Next (0, 0), Exploiting, Q -2.25 → -2.63\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploring, Q -2.49 → -2.68\n",
      "State (0, 1), Action UP, Reward -1, Next (0, 1), Exploiting, Q -2.07 → -2.47\n",
      "State (0, 1), Action DOWN, Reward -1, Next (1, 1), Exploiting, Q -2.21 → -2.33\n",
      "State (1, 1), Action RIGHT, Reward -1, Next (1, 2), Exploiting, Q -1.61 → -1.70\n",
      "State (1, 2), Action DOWN, Reward -1, Next (2, 2), Exploring, Q -0.88 → -1.16\n",
      "State (2, 2), Action UP, Reward -1, Next (1, 2), Exploring, Q -0.50 → -1.25\n",
      "State (1, 2), Action RIGHT, Reward -1, Next (1, 3), Exploiting, Q -1.10 → 13.74\n",
      "State (1, 3), Action DOWN, Reward -1, Next (2, 3), Exploiting, Q 32.88 → 49.69\n",
      "State (2, 3), Action DOWN, Reward 100, Next (3, 3), Exploiting, Q 75.00 → 87.50\n",
      "\n",
      "Episode 14\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploring, Q -2.32 → -2.47\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q -1.79 → -2.04\n",
      "State (2, 0), Action LEFT, Reward -1, Next (2, 0), Exploiting, Q -1.43 → -1.85\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q -1.53 → 2.77\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 8.96 → 23.89\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 44.25 → 55.38\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 75.00 → 87.50\n",
      "\n",
      "Episode 15\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploring, Q -2.47 → -2.58\n",
      "State (1, 0), Action UP, Reward -1, Next (0, 0), Exploiting, Q -1.87 → -2.59\n",
      "State (0, 0), Action UP, Reward -1, Next (0, 0), Exploring, Q -2.63 → -2.98\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -2.58 → -2.71\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q -2.04 → -0.27\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 2.77 → 11.64\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 23.89 → 36.37\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 55.38 → 66.56\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 87.50 → 93.75\n",
      "\n",
      "Episode 16\n",
      "State (0, 0), Action LEFT, Reward -1, Next (0, 0), Exploiting, Q -2.63 → -3.00\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploiting, Q -2.68 → -2.89\n",
      "State (0, 1), Action UP, Reward -1, Next (0, 1), Exploring, Q -2.47 → -2.78\n",
      "State (0, 1), Action DOWN, Reward -1, Next (1, 1), Exploiting, Q -2.33 → -2.43\n",
      "State (1, 1), Action RIGHT, Reward -1, Next (1, 2), Exploiting, Q -1.70 → 4.84\n",
      "State (1, 2), Action RIGHT, Reward -1, Next (1, 3), Exploiting, Q 13.74 → 28.73\n",
      "State (1, 3), Action DOWN, Reward -1, Next (2, 3), Exploiting, Q 49.69 → 63.72\n",
      "State (2, 3), Action DOWN, Reward 100, Next (3, 3), Exploiting, Q 87.50 → 93.75\n",
      "\n",
      "Episode 17\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -2.71 → -1.98\n",
      "State (1, 0), Action UP, Reward -1, Next (0, 0), Exploring, Q -2.59 → -2.69\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -1.98 → -1.61\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q -0.27 → 4.60\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 11.64 → 21.68\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 36.37 → 47.64\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 66.56 → 74.97\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 93.75 → 96.88\n",
      "\n",
      "Episode 18\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q -1.61 → 0.76\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 4.60 → 11.56\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 21.68 → 31.78\n",
      "State (3, 0), Action UP, Reward -1, Next (2, 0), Exploring, Q -1.26 → 13.17\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 31.78 → 36.82\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 47.64 → 57.05\n",
      "State (3, 1), Action LEFT, Reward -1, Next (3, 0), Exploring, Q -0.83 → 24.76\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 57.05 → 61.76\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 74.97 → 80.58\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 96.88 → 98.44\n",
      "\n",
      "Episode 19\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 0.76 → 5.08\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 11.56 → 21.85\n",
      "State (2, 0), Action LEFT, Reward -1, Next (2, 0), Exploring, Q -1.85 → 15.14\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 36.82 → 45.71\n",
      "State (3, 0), Action LEFT, Reward -1, Next (3, 0), Exploring, Q -1.43 → 26.58\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 61.76 → 66.64\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 80.58 → 84.09\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 98.44 → 99.22\n",
      "\n",
      "Episode 20\n",
      "State (0, 0), Action LEFT, Reward -1, Next (0, 0), Exploring, Q -3.00 → 0.29\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 5.08 → 11.87\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 21.85 → 30.99\n",
      "State (2, 0), Action RIGHT, Reward -10, Next (2, 1), Exploring, Q -7.50 → -8.75\n",
      "\n",
      "Episode 21\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 11.87 → 19.38\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 30.99 → 35.56\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 45.71 → 52.34\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 66.64 → 70.66\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 84.09 → 86.19\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 99.22 → 99.61\n",
      "\n",
      "Episode 22\n",
      "State (0, 0), Action UP, Reward -1, Next (0, 0), Exploring, Q -2.98 → 6.73\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 19.38 → 25.20\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 35.56 → 40.84\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 52.34 → 57.47\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 70.66 → 73.62\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 86.19 → 87.42\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 99.61 → 99.80\n",
      "\n",
      "Episode 23\n",
      "State (0, 0), Action LEFT, Reward -1, Next (0, 0), Exploring, Q 0.29 → 10.98\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 25.20 → 30.47\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 40.84 → 45.78\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 57.47 → 61.36\n",
      "State (3, 0), Action LEFT, Reward -1, Next (3, 0), Exploring, Q 26.58 → 45.92\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 73.62 → 75.65\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 87.42 → 88.12\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 99.80 → 99.90\n",
      "\n",
      "Episode 24\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 30.47 → 35.34\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 45.78 → 50.00\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 61.36 → 64.22\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 75.65 → 76.98\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 88.12 → 88.52\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 99.90 → 99.95\n",
      "\n",
      "Episode 25\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 35.34 → 39.67\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 50.00 → 53.40\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 64.22 → 66.25\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 76.98 → 77.82\n",
      "State (3, 1), Action LEFT, Reward -1, Next (3, 0), Exploring, Q 24.76 → 46.90\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 77.82 → 78.24\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 88.52 → 88.74\n",
      "State (3, 2), Action LEFT, Reward -1, Next (3, 1), Exploring, Q 0.00 → 39.43\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 88.74 → 88.85\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 99.95 → 99.98\n",
      "\n",
      "Episode 26\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 39.67 → 43.36\n",
      "State (1, 0), Action LEFT, Reward -1, Next (1, 0), Exploring, Q -2.42 → 22.32\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 53.40 → 56.01\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 66.25 → 67.84\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 78.24 → 78.60\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 88.85 → 88.91\n",
      "State (3, 2), Action LEFT, Reward -1, Next (3, 1), Exploring, Q 39.43 → 59.23\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 88.91 → 88.95\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 99.98 → 99.99\n",
      "\n",
      "Episode 27\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 43.36 → 46.39\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 56.01 → 58.03\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 67.84 → 68.79\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 78.60 → 78.83\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 88.95 → 88.97\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 99.99 → 99.99\n",
      "\n",
      "Episode 28\n",
      "State (0, 0), Action RIGHT, Reward -1, Next (0, 1), Exploring, Q -2.89 → -3.04\n",
      "State (0, 1), Action DOWN, Reward -1, Next (1, 1), Exploiting, Q -2.43 → 0.46\n",
      "State (1, 1), Action RIGHT, Reward -1, Next (1, 2), Exploiting, Q 4.84 → 14.85\n",
      "State (1, 2), Action RIGHT, Reward -1, Next (1, 3), Exploiting, Q 28.73 → 42.54\n",
      "State (1, 3), Action LEFT, Reward -1, Next (1, 2), Exploring, Q -0.72 → 18.28\n",
      "State (1, 2), Action RIGHT, Reward -1, Next (1, 3), Exploiting, Q 42.54 → 49.44\n",
      "State (1, 3), Action DOWN, Reward -1, Next (2, 3), Exploiting, Q 63.72 → 73.55\n",
      "State (2, 3), Action DOWN, Reward 100, Next (3, 3), Exploiting, Q 93.75 → 96.88\n",
      "\n",
      "Episode 29\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 46.39 → 48.81\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 58.03 → 59.47\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 68.79 → 69.37\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 78.83 → 78.95\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 88.97 → 88.98\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 99.99 → 100.00\n",
      "\n",
      "Episode 30\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploring, Q 48.81 → 50.67\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 59.47 → 60.45\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 69.37 → 69.71\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 78.95 → 79.02\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 88.98 → 88.99\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 100.00 → 100.00\n",
      "\n",
      "Episode 31\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 50.67 → 52.04\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 60.45 → 61.09\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 69.71 → 69.91\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploring, Q 79.02 → 79.05\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 88.99 → 88.99\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 100.00 → 100.00\n",
      "\n",
      "Episode 32\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 52.04 → 53.01\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 61.09 → 61.51\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 69.91 → 70.03\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 79.05 → 79.07\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 88.99 → 89.00\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 100.00 → 100.00\n",
      "\n",
      "Episode 33\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 53.01 → 53.68\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 61.51 → 61.77\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 70.03 → 70.10\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 79.07 → 79.09\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 89.00 → 89.00\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 100.00 → 100.00\n",
      "\n",
      "Episode 34\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 53.68 → 54.14\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 61.77 → 61.93\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 70.10 → 70.14\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 79.09 → 79.09\n",
      "State (3, 1), Action UP, Reward -10, Next (2, 1), Exploring, Q -5.00 → -7.50\n",
      "\n",
      "Episode 35\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 54.14 → 54.44\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 61.93 → 62.03\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 70.14 → 70.16\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 79.09 → 79.10\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 89.00 → 89.00\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 100.00 → 100.00\n",
      "\n",
      "Episode 36\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 54.44 → 54.63\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 62.03 → 62.08\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 70.16 → 70.17\n",
      "State (3, 0), Action LEFT, Reward -1, Next (3, 0), Exploring, Q 45.92 → 58.05\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 79.10 → 79.10\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 89.00 → 89.00\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 100.00 → 100.00\n",
      "\n",
      "Episode 37\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 54.63 → 54.75\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 62.08 → 62.12\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 70.17 → 70.18\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 79.10 → 79.10\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 89.00 → 89.00\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 100.00 → 100.00\n",
      "\n",
      "Episode 38\n",
      "State (0, 0), Action LEFT, Reward -1, Next (0, 0), Exploring, Q 10.98 → 29.63\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 54.75 → 54.83\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 62.12 → 62.14\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 70.18 → 70.18\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 79.10 → 79.10\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 89.00 → 89.00\n",
      "State (3, 2), Action UP, Reward -1, Next (2, 2), Exploring, Q -0.50 → -0.97\n",
      "State (2, 2), Action DOWN, Reward -1, Next (3, 2), Exploiting, Q -0.50 → 44.25\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 100.00 → 100.00\n",
      "\n",
      "Episode 39\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 54.83 → 54.88\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 62.14 → 62.15\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 70.18 → 70.19\n",
      "State (3, 0), Action DOWN, Reward -1, Next (3, 0), Exploring, Q -0.97 → 34.61\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 79.10 → 79.10\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 89.00 → 89.00\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 100.00 → 100.00\n",
      "\n",
      "Episode 40\n",
      "State (0, 0), Action DOWN, Reward -1, Next (1, 0), Exploiting, Q 54.88 → 54.91\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 62.15 → 62.16\n",
      "State (2, 0), Action LEFT, Reward -1, Next (2, 0), Exploring, Q 15.14 → 38.66\n",
      "State (2, 0), Action UP, Reward -1, Next (1, 0), Exploring, Q -1.76 → 26.59\n",
      "State (1, 0), Action DOWN, Reward -1, Next (2, 0), Exploiting, Q 62.16 → 62.16\n",
      "State (2, 0), Action DOWN, Reward -1, Next (3, 0), Exploiting, Q 70.19 → 70.19\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 79.10 → 79.10\n",
      "State (3, 1), Action LEFT, Reward -1, Next (3, 0), Exploring, Q 46.90 → 58.55\n",
      "State (3, 0), Action RIGHT, Reward -1, Next (3, 1), Exploiting, Q 79.10 → 79.10\n",
      "State (3, 1), Action RIGHT, Reward -1, Next (3, 2), Exploiting, Q 89.00 → 89.00\n",
      "State (3, 2), Action RIGHT, Reward 100, Next (3, 3), Exploiting, Q 100.00 → 100.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def step(state, action):\n",
    "    x, y = state\n",
    "\n",
    "    if action == 0 and x > 0:\n",
    "        x -= 1\n",
    "    elif action == 1 and x < GridSize - 1:\n",
    "        x += 1\n",
    "    elif action == 2 and y > 0:\n",
    "        y -= 1\n",
    "    elif action == 3 and y < GridSize - 1:\n",
    "        y += 1\n",
    "\n",
    "    next_state = (x, y)\n",
    "\n",
    "    if next_state == Goal:\n",
    "        reward = 100\n",
    "    elif next_state in Trap:\n",
    "        reward = -10\n",
    "    else:\n",
    "        reward = -1\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def draw_grid(state=None, path=None, final=False):\n",
    "    grid = np.ones((GridSize, GridSize, 3))\n",
    "\n",
    "    for i in range(GridSize):\n",
    "        for j in range(GridSize):\n",
    "            if (i, j) == Goal:\n",
    "                grid[i, j] = [0, 1, 0]      # goal\n",
    "            elif (i, j) in Trap:\n",
    "                grid[i, j] = [1, 0, 0]      # trap\n",
    "            elif path and (i, j) in path:\n",
    "                grid[i, j] = [1, 1, 0]      # learned path\n",
    "            elif state is not None and (i, j) == state:\n",
    "                grid[i, j] = [0, 0, 1]      # agent\n",
    "\n",
    "    ax.clear()\n",
    "    ax.imshow(grid)\n",
    "    ax.set_xticks(np.arange(-0.5, GridSize, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, GridSize, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.grid(True)\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "\n",
    "    if not final:\n",
    "        time.sleep(Delay)\n",
    "\n",
    "\n",
    "for ep in range(Episodes): # Training the Q-Learn Algorithm\n",
    "    state = (0, 0)\n",
    "    print(f\"\\nEpisode {ep+1}\")\n",
    "\n",
    "    while state != Goal and state not in Trap:\n",
    "\n",
    "        if random.random() < Epsilon:\n",
    "            action = random.randint(0, 3)\n",
    "            mode = \"Exploring\"\n",
    "        else:\n",
    "            if np.all(Q[state[0], state[1]] == 0):\n",
    "                action = random.randint(0, 3)\n",
    "                mode = \"Exploring\"\n",
    "            else:\n",
    "                action = np.argmax(Q[state[0], state[1]])\n",
    "                mode = \"Exploiting\"\n",
    "\n",
    "        next_state, reward = step(state, action)\n",
    "\n",
    "        old_q = Q[state[0], state[1], action]\n",
    "        Q[state[0], state[1], action] += Alpha * (\n",
    "            reward + Gamma * np.max(Q[next_state[0], next_state[1]])\n",
    "            - Q[state[0], state[1], action]\n",
    "        )\n",
    "\n",
    "        draw_grid(next_state)\n",
    "\n",
    "        print(f\"State {state}, Action {ActionsList[action]}, \"\n",
    "              f\"Reward {reward}, Next {next_state}, \"\n",
    "              f\"{mode}, Q {old_q:.2f} → {Q[state[0],state[1],action]:.2f}\")\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "state = (0, 0) # Final Learned Path starts here\n",
    "optimal_path = [state]\n",
    "\n",
    "while state != Goal:\n",
    "    action = np.argmax(Q[state[0], state[1]])\n",
    "\n",
    "    if action == 0 and state[0] > 0:\n",
    "        state = (state[0]-1, state[1])\n",
    "    elif action == 1 and state[0] < GridSize-1:\n",
    "        state = (state[0]+1, state[1])\n",
    "    elif action == 2 and state[1] > 0:\n",
    "        state = (state[0], state[1]-1)\n",
    "    elif action == 3 and state[1] < GridSize-1:\n",
    "        state = (state[0], state[1]+1)\n",
    "\n",
    "    if state in optimal_path:\n",
    "        break\n",
    "\n",
    "    optimal_path.append(state)\n",
    "\n",
    "\n",
    "draw_grid(state=None, path=optimal_path, final=True)\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
